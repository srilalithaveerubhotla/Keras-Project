{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Auto Differentiation MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srilalithaveerubhotla/Keras-Project/blob/master/Auto_Differentiation_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJdY83zUupFu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPdrQyEdngH1",
        "colab_type": "text"
      },
      "source": [
        "## simple Autograd example\n",
        "explaning from scratch "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3d7j9g7lqz0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# layer class \n",
        "class Layer:\n",
        "\n",
        "    def Forward(self):\n",
        "        raise NotImplemented\n",
        "    \n",
        "    def Backward(self, grad):\n",
        "        raise NotImplemented\n",
        "\n",
        "    def __call__(self, *args):\n",
        "        return self.Forward(*args)\n",
        "\n",
        "\n",
        "# a simple logistic layer for the output\n",
        "class Sigmoid:\n",
        "\n",
        "    def Forward(self,x):\n",
        "        self.x = x   \n",
        "        return 1/(1+np.exp(-x))\n",
        "      \n",
        "    def Backward(self, grad):\n",
        "        grad_input = self.x*(1-self.x) * grad\n",
        "        return grad_input\n",
        "\n",
        "# a simple relu layer\n",
        "class Relu(Layer):\n",
        "\n",
        "    def Forward(self,x):\n",
        "        self.x = x\n",
        "        return np.maximum(np.zeros_like(x), x)\n",
        "      \n",
        "    def Backward(self, grad):\n",
        "        grad_input = (self.x > 0) * grad\n",
        "        return grad_input\n",
        "\n",
        "# softmax class\n",
        "class SoftmaxCrossentropyWithLogits(Layer):\n",
        "\n",
        "    def Forward(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "\n",
        "        exps = np.exp(x) \n",
        "        self.softmax = exps / np.sum(exps, axis=-1, keepdims=True)\n",
        "\n",
        "        logits = self.softmax[np.arange(x.shape[0]),y]\n",
        "        log_likelihood = -np.log(logits)\n",
        "        loss = np.sum(log_likelihood) / x.shape[0]\n",
        "        return loss\n",
        "      \n",
        "    def Backward(self, grad=True):\n",
        "        batch = self.x.shape[0]\n",
        "        grad = self.softmax\n",
        "        grad[np.arange(batch),self.y] -= 1\n",
        "        grad = grad/batch\n",
        "        return grad\n",
        "\n",
        "# mean square error class layer\n",
        "class MSE(Layer):\n",
        "\n",
        "    # forward propagaation layer function\n",
        "    def Forward(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        return ((x - y)**2) / (self.x.shape[0]*2)\n",
        "\n",
        "      #backward propagation layer function\n",
        "    def Backward(self, grad=None):\n",
        "        # 1/2n * Sum(xi-yi)**2 \n",
        "        # dx = 1/2n * Sum( x**2 -2*x*y + y**2) \n",
        "        # dx  = (2x - 2y) / 2*n = (x - y) / n\n",
        "        return (self.x - self.y) / self.x.shape[0]\n",
        "\n",
        "# linear layer class\n",
        "class Linear(Layer):\n",
        "\n",
        "    def __init__(self, input, output, lr=0.0001):\n",
        "        self.A = 2*np.random.random((input, output)) - 1\n",
        "        self.b = 2*np.random.random((output)) - 1\n",
        "        self.lr = lr\n",
        "\n",
        "    # forward propagaation layer function\n",
        "    def Forward(self, x):\n",
        "        self.x = x\n",
        "        return np.dot(x,self.A) + self.b\n",
        "\n",
        "    #backward propagation layer function\n",
        "    def Backward(self, grad):\n",
        "        # d_layer / db = 1\n",
        "        b_grad = grad.mean(axis=0)*self.x.shape[0]\n",
        "        # d_layer / dA = x\n",
        "        A_grad = np.dot(self.x.T, grad)\n",
        "        # As this layer have somee weights we need to update them using \n",
        "        # gradient descent\n",
        "        # compute df / dx = df / d_layer * d_layer / dx\n",
        "        # df / d_layer == grad\n",
        "        grad_input = np.dot(grad, self.A.T)\n",
        "        \n",
        "        self.A -= A_grad * self.lr\n",
        "        self.b -= b_grad * self.lr\n",
        "\n",
        "        return grad_input\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phKYPifgFxSP",
        "colab_type": "text"
      },
      "source": [
        "## Auto Grad example on MNIST classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlcCxptFlHMv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision.datasets import MNIST\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class Model(Layer):\n",
        "\n",
        "    def __init__(self, lr=0.00001):\n",
        "        self.lr = lr\n",
        "        self.layers = [\n",
        "            Linear(784,100, lr=self.lr),\n",
        "            Relu(),\n",
        "            Linear(100,200, lr=self.lr),\n",
        "            Relu(),\n",
        "            Linear(200,10, lr=self.lr)        \n",
        "        ]\n",
        "    # forward propagaation layer function\n",
        "    def Forward(self,x):\n",
        "        for l in self.layers:\n",
        "            x = l(x)\n",
        "        return x\n",
        "\n",
        "    #backward propagation layer function\n",
        "    def Backward(self, grad):\n",
        "        for l in self.layers[::-1]:\n",
        "            grad = l.Backward(grad)\n",
        "\n",
        "        return grad\n",
        "\n",
        "\n",
        "simple = transforms.Compose([\n",
        "    transforms.ToTensor(), # converts to [0,1] interval\n",
        "])\n",
        "# importing mnist dataset \n",
        "ds = MNIST('./mnist', download=True, transform=simple)\n",
        "ld = DataLoader(ds, batch_size=2, pin_memory=True, drop_last=True) \n",
        "\n",
        "mm = Model()\n",
        "# using softmax entropy function\n",
        "loss = SoftmaxCrossentropyWithLogits()\n",
        "_loss_avg = 0 \n",
        "for e in range(7):\n",
        "    for i, (img, label) in enumerate(ld):\n",
        "        x = img.view(2,-1).numpy()\n",
        "\n",
        "        res = mm(x)\n",
        "        _loss = loss(res, label.numpy())\n",
        "        _loss_avg += _loss.mean() # running loss mean\n",
        "        grad = loss.Backward(1)\n",
        "        mm.Backward(grad)\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(_loss_avg/100)\n",
        "            _loss_avg = 0\n",
        "            print('---------')\n",
        "            \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQixrHLTlH8e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}